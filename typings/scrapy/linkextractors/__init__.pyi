"""
This type stub file was generated by pyright.
"""

import re
from six.moves.urllib.parse import urlparse
from parsel.csstranslator import HTMLTranslator
from w3lib.url import canonicalize_url
from scrapy.utils.misc import arg_to_iter
from scrapy.utils.url import url_has_any_extension, url_is_from_any_domain
from .lxmlhtml import LxmlLinkExtractor as LinkExtractor

"""
scrapy.linkextractors

This package contains a collection of Link Extractors.

For more info see docs/topics/link-extractors.rst
"""
IGNORED_EXTENSIONS = ['mng', 'pct', 'bmp', 'gif', 'jpg', 'jpeg', 'png', 'pst', 'psp', 'tif', 'tiff', 'ai', 'drw', 'dxf', 'eps', 'ps', 'svg', 'mp3', 'wma', 'ogg', 'wav', 'ra', 'aac', 'mid', 'au', 'aiff', '3gp', 'asf', 'asx', 'avi', 'mov', 'mp4', 'mpg', 'qt', 'rm', 'swf', 'wmv', 'm4a', 'm4v', 'flv', 'xls', 'xlsx', 'ppt', 'pptx', 'pps', 'doc', 'docx', 'odt', 'ods', 'odg', 'odp', 'css', 'pdf', 'exe', 'bin', 'rss', 'zip', 'rar']
_re_type = type(re.compile("", 0))
_matches = lambda url, regexs: any(r.search(url) for r in regexs)
_is_valid_url = lambda url: url.split('://', 1)[0] in 'http', 'https', 'file', 'ftp'
class FilteringLinkExtractor(object):
    _csstranslator = ...
    def __init__(self, link_extractor, allow, deny, allow_domains, deny_domains, restrict_xpaths, canonicalize, deny_extensions, restrict_css, restrict_text):
        self.link_extractor = ...
        self.allow_res = ...
        self.deny_res = ...
        self.allow_domains = ...
        self.deny_domains = ...
        self.restrict_xpaths = ...
        self.canonicalize = ...
        self.deny_extensions = ...
        self.restrict_text = ...
    
    def _link_allowed(self, link):
        ...
    
    def matches(self, url):
        ...
    
    def _process_links(self, links):
        ...
    
    def _extract_links(self, *args, **kwargs):
        ...
    


