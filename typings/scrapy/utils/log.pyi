"""
This type stub file was generated by pyright.
"""

import logging
from types import TracebackType
from typing import Any, List, Optional, TYPE_CHECKING, Tuple, Type, Union
from twisted.python.failure import Failure
from scrapy.settings import Settings
from scrapy.crawler import Crawler

if TYPE_CHECKING:
    ...
logger = ...
def failure_to_exc_info(failure: Failure) -> Optional[Tuple[Type[BaseException], BaseException, Optional[TracebackType]]]:
    """Extract exc_info from Failure instances"""
    ...

class TopLevelFormatter(logging.Filter):
    """Keep only top level loggers's name (direct children from root) from
    records.

    This filter will replace Scrapy loggers' names with 'scrapy'. This mimics
    the old Scrapy log behaviour and helps shortening long names.

    Since it can't be set for just one logger (it won't propagate for its
    children), it's going to be set in the root handler, with a parametrized
    ``loggers`` list where it should act.
    """
    def __init__(self, loggers: Optional[List[str]] = ...) -> None:
        ...
    
    def filter(self, record: logging.LogRecord) -> bool:
        ...
    


DEFAULT_LOGGING = ...
def configure_logging(settings: Union[Settings, dict, None] = ..., install_root_handler: bool = ...) -> None:
    """
    Initialize logging defaults for Scrapy.

    :param settings: settings used to create and configure a handler for the
        root logger (default: None).
    :type settings: dict, :class:`~scrapy.settings.Settings` object or ``None``

    :param install_root_handler: whether to install root logging handler
        (default: True)
    :type install_root_handler: bool

    This function does:

    - Route warnings and twisted logging through Python standard logging
    - Assign DEBUG and ERROR level to Scrapy and Twisted loggers respectively
    - Route stdout to log if LOG_STDOUT setting is True

    When ``install_root_handler`` is True (default), this function also
    creates a handler for the root logger according to given settings
    (see :ref:`topics-logging-settings`). You can override default options
    using ``settings`` argument. When ``settings`` is empty or None, defaults
    are used.
    """
    ...

_scrapy_root_handler: Optional[logging.Handler] = ...
def install_scrapy_root_handler(settings: Settings) -> None:
    ...

def get_scrapy_root_handler() -> Optional[logging.Handler]:
    ...

def log_scrapy_info(settings: Settings) -> None:
    ...

def log_reactor_info() -> None:
    ...

class StreamLogger:
    """Fake file-like stream object that redirects writes to a logger instance

    Taken from:
        https://www.electricmonk.nl/log/2011/08/14/redirect-stdout-and-stderr-to-a-logger-in-python/
    """
    def __init__(self, logger: logging.Logger, log_level: int = ...) -> None:
        ...
    
    def write(self, buf: str) -> None:
        ...
    
    def flush(self) -> None:
        ...
    


class LogCounterHandler(logging.Handler):
    """Record log levels count into a crawler stats"""
    def __init__(self, crawler: Crawler, *args: Any, **kwargs: Any) -> None:
        ...
    
    def emit(self, record: logging.LogRecord) -> None:
        ...
    


def logformatter_adapter(logkws: dict) -> Tuple[int, str, dict]:
    """
    Helper that takes the dictionary output from the methods in LogFormatter
    and adapts it into a tuple of positional arguments for logger.log calls,
    handling backward compatibility as well.
    """
    ...

