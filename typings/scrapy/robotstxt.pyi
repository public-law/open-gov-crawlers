"""
This type stub file was generated by pyright.
"""

from abc import ABCMeta, abstractmethod

logger = ...
def decode_robotstxt(robotstxt_body, spider, to_native_str_type=...): # -> str:
    ...

class RobotParser(metaclass=ABCMeta):
    @classmethod
    @abstractmethod
    def from_crawler(cls, crawler, robotstxt_body): # -> None:
        """Parse the content of a robots.txt_ file as bytes. This must be a class method.
        It must return a new instance of the parser backend.

        :param crawler: crawler which made the request
        :type crawler: :class:`~scrapy.crawler.Crawler` instance

        :param robotstxt_body: content of a robots.txt_ file.
        :type robotstxt_body: bytes
        """
        ...
    
    @abstractmethod
    def allowed(self, url, user_agent): # -> None:
        """Return ``True`` if  ``user_agent`` is allowed to crawl ``url``, otherwise return ``False``.

        :param url: Absolute URL
        :type url: str

        :param user_agent: User agent
        :type user_agent: str
        """
        ...
    


class PythonRobotParser(RobotParser):
    def __init__(self, robotstxt_body, spider) -> None:
        ...
    
    @classmethod
    def from_crawler(cls, crawler, robotstxt_body): # -> Self@PythonRobotParser:
        ...
    
    def allowed(self, url, user_agent): # -> bool:
        ...
    


class ReppyRobotParser(RobotParser):
    def __init__(self, robotstxt_body, spider) -> None:
        ...
    
    @classmethod
    def from_crawler(cls, crawler, robotstxt_body): # -> Self@ReppyRobotParser:
        ...
    
    def allowed(self, url, user_agent):
        ...
    


class RerpRobotParser(RobotParser):
    def __init__(self, robotstxt_body, spider) -> None:
        ...
    
    @classmethod
    def from_crawler(cls, crawler, robotstxt_body): # -> Self@RerpRobotParser:
        ...
    
    def allowed(self, url, user_agent):
        ...
    


class ProtegoRobotParser(RobotParser):
    def __init__(self, robotstxt_body, spider) -> None:
        ...
    
    @classmethod
    def from_crawler(cls, crawler, robotstxt_body): # -> Self@ProtegoRobotParser:
        ...
    
    def allowed(self, url, user_agent):
        ...
    


