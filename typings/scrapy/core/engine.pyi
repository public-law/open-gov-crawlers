"""
This type stub file was generated by pyright.
"""

import logging
from twisted.internet import defer
from scrapy import signals

"""
This is the Scrapy engine which controls the Scheduler, Downloader and Spiders.

For more information see docs/topics/architecture.rst

"""
logger = logging.getLogger(__name__)
class Slot(object):
    def __init__(self, start_requests, close_if_idle, nextcall, scheduler):
        self.closing = ...
        self.inprogress = ...
        self.start_requests = ...
        self.close_if_idle = ...
        self.nextcall = ...
        self.scheduler = ...
        self.heartbeat = ...
    
    def add_request(self, request):
        ...
    
    def remove_request(self, request):
        ...
    
    def close(self):
        self.closing = ...
    
    def _maybe_fire_closing(self):
        ...
    


class ExecutionEngine(object):
    def __init__(self, crawler, spider_closed_callback):
        self.crawler = ...
        self.settings = ...
        self.signals = ...
        self.logformatter = ...
        self.slot = ...
        self.spider = ...
        self.running = ...
        self.paused = ...
        self.scheduler_cls = ...
        self.downloader = ...
        self.scraper = ...
    
    @defer.inlineCallbacks
    def start(self):
        """Start the execution engine"""
        self.start_time = ...
        self.running = ...
    
    def stop(self):
        """Stop the execution engine gracefully"""
        self.running = ...
    
    def close(self):
        """Close the execution engine gracefully.

        If it has already been started, stop it. In all cases, close all spiders
        and the downloader.
        """
        ...
    
    def pause(self):
        """Pause the execution engine"""
        self.paused = ...
    
    def unpause(self):
        """Resume the execution engine"""
        self.paused = ...
    
    def _next_request(self, spider):
        ...
    
    def _needs_backout(self, spider):
        ...
    
    def _next_request_from_scheduler(self, spider):
        ...
    
    def _handle_downloader_output(self, response, request, spider):
        ...
    
    def spider_is_idle(self, spider):
        ...
    
    @property
    def open_spiders(self):
        ...
    
    def has_capacity(self):
        """Does the engine have capacity to handle more spiders"""
        ...
    
    def crawl(self, request, spider):
        ...
    
    def schedule(self, request, spider):
        ...
    
    def download(self, request, spider):
        ...
    
    def _downloaded(self, response, slot, request, spider):
        ...
    
    def _download(self, request, spider):
        ...
    
    @defer.inlineCallbacks
    def open_spider(self, spider, start_requests=..., close_if_idle: bool = ...):
        self.slot = ...
        self.spider = ...
    
    def _spider_idle(self, spider):
        """Called when a spider gets idle. This function is called when there
        are no remaining pages to download or schedule. It can be called
        multiple times. If some extension raises a DontCloseSpider exception
        (in the spider_idle signal handler) the spider is not closed until the
        next loop and this function is guaranteed to be called (at least) once
        again for this spider.
        """
        ...
    
    def close_spider(self, spider, reason=...):
        """Close (cancel) spider and clear all its outstanding requests"""
        ...
    
    def _close_all_spiders(self):
        ...
    
    @defer.inlineCallbacks
    def _finish_stopping_engine(self):
        ...
    


